{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0763  0.1235  0.2544  0.4612  0.5312  0.874   0.3777  0.8735  0.3633\n",
      " 0.7983  0.9287  0.971   0.9937  0.992   0.9897  0.983   0.989   0.991\n",
      " 0.9893  0.9688  0.808   0.8687  0.926   0.588   0.6685  0.614   0.4863\n",
      " 0.887   0.9834  0.97    0.9585  0.953   0.984   0.966   0.9497  0.9766\n",
      " 0.944   0.952   0.943   0.915   0.9316  0.8594  0.5786  0.424   0.3486\n",
      " 0.2622  0.3398  0.3137  0.283   0.2893  0.2218  0.1947  0.4055  0.2644\n",
      " 0.7085  0.721   0.961   0.9907  0.979   0.971   0.963   0.9683  0.792\n",
      " 0.6714  0.7046  0.5254  0.5977  0.826   0.7437  0.615   0.8     0.8945\n",
      " 0.8438  0.7935  0.849   0.889   0.5747  0.273   0.7227  0.1622  0.2527\n",
      " 0.258   0.3875  0.3535  0.4724  0.458   0.3364  0.2769  0.3774  0.2866\n",
      " 0.1803  0.315   0.3628  0.2186  0.3477  0.5493  0.519   0.5347  0.465\n",
      " 0.7666  0.801   0.4226  0.783   0.9053  0.9326  0.579   0.395   0.3943\n",
      " 0.3418  0.434   0.2654  0.3457  0.1453  0.1099  0.2424  0.21    0.9814\n",
      " 0.9917  0.9473  0.9844  0.961   0.54    0.3667  0.3564  0.3423  0.2244\n",
      " 0.3193  0.2462  0.3147  0.3376  0.08466 0.2605  0.2915  0.2455  0.113\n",
      " 0.343   0.3064  0.3691  0.277   0.3367  0.2437  0.2024  0.3193  0.3645\n",
      " 0.9644  0.974   0.986   0.9893  0.828   0.723   0.5894  0.4194  0.5737\n",
      " 0.4993  0.3462  0.1569  0.312   0.3694  0.1962  0.1697  0.1654  0.3245\n",
      " 0.2996  0.3386  0.3586  0.2316  0.2917  0.2896  0.1647  0.1332 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import finetuning_utils\n",
    "\n",
    "# some constants\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MAX_LENGTH = 1024\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUT = 0.25\n",
    "\n",
    "# UPDATE THIS!\n",
    "MODEL_PATH = '/home/skrhakv/nn-for-kamila/model.pt'\n",
    "\n",
    "# define the model - if we do not define the model then the loading of the model will fail\n",
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "        self.plDDT_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        return self.classifier(token_embeddings), self.plDDT_regressor(token_embeddings), self.distance_regressor(token_embeddings)\n",
    "\n",
    "# load the model\n",
    "model = torch.load(MODEL_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "KRAS_sequence = 'GMTEYKLVVVGACGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETSLLDILDTAGQEEYSAMRDQYMRTGEGFLLVFAINNTKSFEDIHHYREQIKRVKDSEDVPMVLVGNKSDLPSRTVDTKQAQDLARSYGIPFIETSAKTRQGVDDAFYTLVREIRKHKEK'\n",
    "\n",
    "# tokenize the sequence\n",
    "tokenized_sequences = tokenizer(KRAS_sequence, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "tokenized_sequences = {k: torch.tensor([v]).to(DEVICE) for k,v in tokenized_sequences.items()}\n",
    "\n",
    "# predict\n",
    "output = model(tokenized_sequences)\n",
    "output = output.flatten()\n",
    "\n",
    "mask = (tokenized_sequences['attention_mask'] == 1).flatten()\n",
    "\n",
    "output = torch.sigmoid(output[mask][1:-1]).detach().cpu().numpy()\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
